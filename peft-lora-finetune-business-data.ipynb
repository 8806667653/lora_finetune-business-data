{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed452ba",
   "metadata": {},
   "source": [
    "##  Get Insight from your Business Data - Build LLM application with PEFT (with LoRA) using ðŸ¤— Hugging Face by Ashish Kumar Jain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab8e05",
   "metadata": {},
   "source": [
    "### LoRA is one of widely used PEFT method even in most cases when someone says PEFT, they typically mean LoRA only. In the LoRA, we kept original weights of model frozen and inject the small new trainable parameters with low dimensions matrices. Idea here is that we can injects two new low dimension matrices (assume A and B) along side of actual weights matrices. Sizes of these two new matrices can be selected be in such a way so there product (Assume C) should be same dimension  as the size of actual model weights. During the fine tuning of model for a task, all pre-trained model parameters are kept frozen, and only A and B matrices are trainable. Once the fine tuning is completed, we have these new weights which is product of A and B matrix. These weights is now trained for a specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c6108",
   "metadata": {},
   "source": [
    "### In the notebook In the blog we will use ðŸ¤— Hugging Face, it is a platform where machine learning community collaborates on models, datasets and applications. Hugging Face introduce the ðŸ¤— PEFT library, which provides the latest Parameter-Efficient Fine-tuning techniques seamlessly integrated with Transformers. It support LoRA technique as well. We will use this PEFT library for our implementation. We will also use Hugging Face to download one of the open source LLM model FLAN_T5 from Google . We will load this model from the local machine. You can easily download this model from Hugging Face by cloning the model repository. It will help you to run this code without internet or in very constrained environment. Downloading model can take time depending on your network speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176bc693-b33c-433f-8974-39c3fb22c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade pip\n",
    "! pip install 'transformers[torch]'\n",
    "! pip install datasets\n",
    "! pip install evaluate==0.4.0\n",
    "! pip install rouge_score==0.1.2\n",
    "! pip install peft==0.3.0 \n",
    "! pip install loralib==0.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529ac025",
   "metadata": {},
   "source": [
    "#### Hugging Face provides Datasets library for easily accessing and sharing datasets. We can load the dataset in a single line of code from multiple sources (Hugging Face hub, local files systems and memory etc) in different formats (CSV, JSON, parquet, arrow, sql for reading from database etc) and use its powerful data processing methods to quickly get our dataset ready for training with LLM.\n",
    "\n",
    "#### Sample Data Format\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"version\": \"0.1.0\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"Question\": \"Generate a list of three uses of big data?\",\n",
    "            \"Answer\": \"1. Big data can be used to identify patterns and trends in customer behavior.\\\\n2. Big data can be used to improve customer service and experience.\\\\n3. Big data can be used to develop predictive models for marketing and sales.\"\n",
    "        },\n",
    "        {\n",
    "            \"Question\": \"Predict the weather tomorrow morning.?\",\n",
    "            \"Answer\": \"Tomorrow morning is expected to be sunny with temperatures ranging from 15 to 19 degrees Celsius.\"\n",
    "        }\n",
    "   ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d85a736-fd8a-408e-8313-d7f4341b6efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\":\"dataset/GK/train.json\",\n",
    "              \"test\":\"dataset/GK/validation.json\",\n",
    "              \"validation\":\"dataset/GK/test.json\"\n",
    "             }\n",
    "dataset = load_dataset(\"json\",data_files = data_files,field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7cbb3",
   "metadata": {},
   "source": [
    "#### Hugging Face provides tokenizer class which is in charge of preparing the inputs for a model. We will use open source FLAN-T5-LARGE model from Hugging Face and load from local. It is a good encode-decoder instruct model. It shows good capability in many tasks.\n",
    "\n",
    "#### You can easily download flan-t5-large model from Hugging Face by cloning the model repository. \n",
    "#### git clone https://huggingface.co/google/flan-t5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e76c1e9-5e22-486c-8a2c-f29318f9862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "modelPath = \"model/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelPath)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c676ed",
   "metadata": {},
   "source": [
    "#### We will create instructed dataset for the training. We will to convert the question-answer pairs into explicit instructions for the LLM. Lets create a prompt instruction having instruction start and end of prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a311b-1ce3-4005-8563-82af6a38f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_generator(batchData):\n",
    "    start = 'Assuming you are working as General Knowladge instructor. Can you please answer the below question?\\n\\n'\n",
    "    end = '\\n Answer: '\n",
    "    training_prompt = [start + question + end for question in batchData['Question']]\n",
    "    batchData['input_ids'] = tokenizer(training_prompt, padding=\"max_length\", return_tensors=\"pt\").input_ids\n",
    "    batchData['labels'] = tokenizer(batchData['Answer'], padding=\"max_length\", return_tensors=\"pt\").input_ids\n",
    "    return batchData\n",
    "\n",
    "instructed_datasets = dataset.map(prompt_generator, batched=True)\n",
    "instructed_datasets = instructed_datasets.remove_columns(['id','Question', 'Answer'])\n",
    "#print(instructed_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0c9bb2",
   "metadata": {},
   "source": [
    "#### Let's create the LoRA configuration where we will specify rank r as hyper parameter and other configuration parameter. We will then create the PEFT version of base model using which we can train the new LoRA matrices (adapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a9bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,#LoRA scaling factor\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")\n",
    "peft_model = get_peft_model(base_model, \n",
    "                            lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0bd2fa",
   "metadata": {},
   "source": [
    "#### We will use the PyTorch framework for peft/LoRA fine tuning the model. The Hugging Face Trainer class provides an API for feature-complete training in PyTorch for most standard use cases. Before instantiating our Trainer object, we will create a TrainingArguments to access all the points of customization during training. In below code i am using only 1 epoch for model training, you can choose no. of epochs and other training parameter based on your compute, memory available and based on the final model evaluation result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import time\n",
    "\n",
    "output_dir = f'./model/peft-trained-model-output/flan-output-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-3, \n",
    "    num_train_epochs=1, \n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps =1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=instructed_datasets['train'],\n",
    "    eval_dataset=instructed_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2413c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e052309e",
   "metadata": {},
   "source": [
    "#### After training you can save the PEFT model for future evaluation and inference use. if you check saved model, it saves only adapter part of it and size is very less compared to base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ce997",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dir = f'./model/peft-trained-model/flan-trained-{str(int(time.time()))}'\n",
    "tokenizer.save_pretrained(saved_dir)\n",
    "peft_model.save_pretrained(saved_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94763bda",
   "metadata": {},
   "source": [
    "#### We can load the saved PEFT adapter from local file system along with its base FLAN_T5 model. We are passing is_trainable=false as we will use it only for inference not for further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model,'model/peft-trained-model/flan-trained-1693655699', is_trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aafd3ce",
   "metadata": {},
   "source": [
    "#### For Generative AI applications, a qualitative approach where we ask our-self the question \"Is my model behaving right way?\" is usually a good starting point. We can see that by manually seeing the difference between actual answer with answers given by the peft model. We can use our test dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb81876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "import pandas as pd\n",
    "\n",
    "questions = dataset['test']['Question']\n",
    "actual_answers = dataset['test']['Answer']\n",
    "peft_model_answers = []\n",
    "\n",
    "for _, question in enumerate(questions):\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "Assuming you are working as General Knowladge instructor. Can you please answer the below question?\n",
    "\n",
    "{question}\n",
    "Answer:\"\"\";\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "    peft_model_answers.append(peft_model_text_output)\n",
    "    \n",
    "answers = list(zip(questions,actual_answers,peft_model_answers))\n",
    "df = pd.DataFrame(answers, columns = ['question','actual answer','peft model answer'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2ea4f",
   "metadata": {},
   "source": [
    "#### Other evaluating approach is qualitative. The ROUGE metric helps quantify the validity of answers produced by models. It compares answers to a actual answer which is part of our test dataset.You can read more about this from ROUGE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_answers,\n",
    "    references=actual_answers,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf827ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
